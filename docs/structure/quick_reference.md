# å¿«é€Ÿå‚è€ƒæ‰‹å†Œ

> æœ€åæ›´æ–°ï¼š2025-12-31

å¿«é€ŸæŸ¥æ‰¾å¸¸ç”¨å‘½ä»¤ã€é…ç½®å’Œä»£ç ç‰‡æ®µã€‚

---

## ğŸ“‹ ç›®å½•

- [å¯åŠ¨å’Œéƒ¨ç½²](#å¯åŠ¨å’Œéƒ¨ç½²)
- [é…ç½®æ–‡ä»¶](#é…ç½®æ–‡ä»¶)
- [API æ¥å£](#api-æ¥å£)
- [æµ‹è¯•å‘½ä»¤](#æµ‹è¯•å‘½ä»¤)
- [å¸¸ç”¨ä»£ç ç‰‡æ®µ](#å¸¸ç”¨ä»£ç ç‰‡æ®µ)
- [æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥)

---

## å¯åŠ¨å’Œéƒ¨ç½²

### æœ¬åœ°å¼€å‘å¯åŠ¨

```bash
# 1. å¯åŠ¨æœåŠ¡
bash scripts/start_server.sh

# 2. æˆ–è€…æ‰‹åŠ¨å¯åŠ¨
cd /home/parser/code/ai_idea_gen
source venv/bin/activate  # å¦‚æœä½¿ç”¨è™šæ‹Ÿç¯å¢ƒ
uvicorn backend.main:app --host 0.0.0.0 --port 8001 --reload
```

### ç”Ÿäº§ç¯å¢ƒå¯åŠ¨

```bash
# ä½¿ç”¨ gunicorn + uvicorn workers
gunicorn backend.main:app \
  --workers 4 \
  --worker-class uvicorn.workers.UvicornWorker \
  --bind 0.0.0.0:8001
```

### å¥åº·æ£€æŸ¥

```bash
curl http://localhost:8001/health
# è¿”å›: {"status": "ok"}
```

---

## é…ç½®æ–‡ä»¶

### .envï¼ˆç¯å¢ƒå˜é‡ï¼‰

```env
# é£ä¹¦åº”ç”¨é…ç½®
FEISHU_APP_ID=cli_xxxxxxxxxxxxx
FEISHU_APP_SECRET=xxxxxxxxxxxxxxxxxxxxx

# LLM API Keys
GEMINI_API_KEY=xxxxxxxxxxxxxxxxxxxxx
DEEP_RESEARCH_API_KEY=xxxxxxxxxxxxxxxxxxxxx

# ä¸šåŠ¡é…ç½®
PROCESS_TIMEOUT=60
WEBHOOK_OUTPUT_URL=https://example.com/webhook
WEBHOOK_OUTPUT_TIMEOUT_S=10.0
```

### llm_config.ymlï¼ˆLLM é…ç½®ï¼‰

```yaml
# Provider å®šä¹‰
providers:
  primary_gemini:
    type: "openai-compatible"
    base_url: "https://zjuapi.com/v1"
    model: "gemini-2.5-flash-nothinking"
    api_key_env: "GEMINI_API_KEY"

# Chain å®šä¹‰ï¼ˆFallback é“¾ï¼‰
chains:
  idea_expand:
    - provider: "primary_gemini"
      timeout_s: 60
  
  title_generation:
    - provider: "no_thinking_gemini"
      timeout_s: 15

# å…¨å±€é…ç½®
global:
  max_retries_per_provider: 1
  overall_timeout_s: 60
```

### workflow_config.ymlï¼ˆå·¥ä½œæµé…ç½®ï¼‰

```yaml
workflows:
  idea_expand:
    processor: "idea_expander"      # Processor åç§°
    chain: "idea_expand"            # LLM Chain åç§°
    output: "feishu_child_doc"      # OutputHandler åç§°
    notify_user: false              # æ˜¯å¦å‘é€é€šçŸ¥
  
  research:
    processor: "research"
    chain: "research"
    output: "feishu_child_doc"
    notify_user: false
```

---

## API æ¥å£

### åŸºç¡€ä¿¡æ¯

- **Base URL**: `http://localhost:8001`
- **Content-Type**: `application/json`

### ä¸»è¦æ¥å£

#### 1. å¥åº·æ£€æŸ¥

```bash
GET /health
```

**å“åº”**ï¼š
```json
{
  "status": "ok"
}
```

---

#### 2. è§¦å‘æ–‡æ¡£å¤„ç†

```bash
POST /api/addon/process
```

**è¯·æ±‚ä½“**ï¼š
```json
{
  "token": "WTY1wJevAiiSm4kGTbfcboxXnVc",
  "user_id": "ou_xxxxxxxxxxxxx",
  "mode": "idea_expand",
  "trigger_source": "docs_addon"
}
```

**å“åº”**ï¼ˆ202 Acceptedï¼‰ï¼š
```json
{
  "task_id": "7dfc27556c384ec396eb17fa21e7367b",
  "status": "accepted",
  "message": "Processing started"
}
```

---

#### 3. æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€

```bash
GET /api/addon/tasks/{task_id}
```

**å“åº”**ï¼ˆå¤„ç†ä¸­ï¼‰ï¼š
```json
{
  "task_id": "7dfc2755...",
  "status": "running",
  "progress": {
    "stage": "llm",
    "percent": 35,
    "message": "è°ƒç”¨æ¨¡å‹ç”Ÿæˆå†…å®¹"
  },
  "created_at": 1735632000.0
}
```

**å“åº”**ï¼ˆæˆåŠŸï¼‰ï¼š
```json
{
  "task_id": "7dfc2755...",
  "status": "succeeded",
  "result": {
    "child_doc_token": "doccnXXXXXXXXXX",
    "child_doc_url": "https://feishu.cn/docx/doccnXXXXXXXXXX",
    "title": "AIé©±åŠ¨çš„äº§å“åˆ›æ–°æ–¹æ¡ˆ",
    "summary": "æ‰©å±•æ€è·¯å»ºè®®"
  },
  "created_at": 1735632000.0,
  "updated_at": 1735632045.0
}
```

**å“åº”**ï¼ˆå¤±è´¥ï¼‰ï¼š
```json
{
  "task_id": "7dfc2755...",
  "status": "failed",
  "error": "LLM API è°ƒç”¨å¤±è´¥",
  "created_at": 1735632000.0,
  "updated_at": 1735632010.0
}
```

---

#### 4. é£ä¹¦äº‹ä»¶å›è°ƒ

```bash
POST /api/feishu/event
```

**è¯·æ±‚ä½“**ï¼ˆURL éªŒè¯ï¼‰ï¼š
```json
{
  "challenge": "xxx"
}
```

**å“åº”**ï¼š
```json
{
  "challenge": "xxx"
}
```

**è¯·æ±‚ä½“**ï¼ˆäº‹ä»¶æ¨é€ï¼‰ï¼š
```json
{
  "schema": "2.0",
  "header": {
    "event_id": "xxx",
    "event_type": "docx.document.updated_v1"
  },
  "event": {
    "doc_token": "doccnXXXXXXXXXX",
    "operator_id": "ou_xxxxxxxxxxxxx"
  }
}
```

**å“åº”**ï¼š
```json
{
  "code": 0,
  "msg": "ok"
}
```

---

## æµ‹è¯•å‘½ä»¤

### æ‰‹åŠ¨æµ‹è¯•è„šæœ¬

#### åŸºç¡€ç”¨æ³•

```bash
python3 tests/manual_trigger.py \
  --token WTY1wJevAiiSm4kGTbfcboxXnVc \
  --user-id test_user_001 \
  --mode idea_expand
```

#### å®Œæ•´å‚æ•°

```bash
python3 tests/manual_trigger.py \
  --endpoint http://localhost:8001/api/addon/process \
  --token WTY1wJevAiiSm4kGTbfcboxXnVc \
  --user-id test_user_001 \
  --mode idea_expand \
  --trigger-source manual_test \
  --poll-interval 2.0 \
  --poll-timeout 180.0
```

#### æµ‹è¯•æ·±åº¦è°ƒç ”ï¼ˆé•¿æ—¶ä»»åŠ¡ï¼‰

```bash
python3 tests/manual_trigger.py \
  --token WTY1wJevAiiSm4kGTbfcboxXnVc \
  --user-id test_user_001 \
  --mode research \
  --poll-interval 15.0 \
  --poll-timeout 3600.0
```

#### æµ‹è¯• Wiki èŠ‚ç‚¹

```bash
python3 tests/manual_trigger.py \
  --token wikcnXXXXXXXXXX \
  --user-id test_user_001 \
  --mode idea_expand \
  --wiki-space-id 7123456789012345678
```

#### ä¸ç­‰å¾…ç»“æœï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰

```bash
python3 tests/manual_trigger.py \
  --token WTY1wJevAiiSm4kGTbfcboxXnVc \
  --user-id test_user_001 \
  --mode idea_expand \
  --no-wait
```

### ä½¿ç”¨ curl æµ‹è¯•

#### è§¦å‘å¤„ç†

```bash
curl -X POST http://localhost:8001/api/addon/process \
  -H "Content-Type: application/json" \
  -d '{
    "token": "WTY1wJevAiiSm4kGTbfcboxXnVc",
    "user_id": "test_user_001",
    "mode": "idea_expand"
  }'
```

#### æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€

```bash
curl http://localhost:8001/api/addon/tasks/7dfc27556c384ec396eb17fa21e7367b
```

---

## å¸¸ç”¨ä»£ç ç‰‡æ®µ

### æ–°å¢ Processor

```python
# 1. åˆ›å»ºæ–‡ä»¶ï¼šbackend/services/processors/my_processor.py
from backend.services.processors.base import BaseDocProcessor, ProcessorResult
from textwrap import dedent

class MyProcessor(BaseDocProcessor):
    async def run(self, *, doc_content, doc_title, chain, context=None):
        system_prompt = dedent("""
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„...
            è¦æ±‚ï¼š
            - ...
            - ...
        """).strip()
        
        user_prompt = dedent(f"""
            æ–‡æ¡£æ ‡é¢˜ï¼š{doc_title}
            æ–‡æ¡£å†…å®¹ï¼š
            {doc_content}
            
            è¯·...
        """).strip()
        
        result = await self.llm_client.chat_completion(
            chain=chain,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.7
        )
        
        return ProcessorResult(
            title=f"{doc_title} - æˆ‘çš„å¤„ç†",
            content_md=result.strip(),
            summary="å¤„ç†å®Œæˆ",
            metadata={"mode": "my_mode"}
        )

# 2. æ³¨å†Œï¼šbackend/services/processors/registry.py
from backend.services.processors.my_processor import MyProcessor

PROCESSOR_REGISTRY = {
    # ... existing processors ...
    "my_processor": MyProcessor,
}

# 3. é…ç½®ï¼šworkflow_config.yml
workflows:
  my_mode:
    processor: "my_processor"
    chain: "my_chain"
    output: "feishu_child_doc"
    notify_user: true

# 4. LLM é…ç½®ï¼šllm_config.yml
chains:
  my_chain:
    - provider: "primary_gemini"
      timeout_s: 60
```

### æ–°å¢ OutputHandler

```python
# 1. åˆ›å»ºæ–‡ä»¶ï¼šbackend/services/outputs/my_output.py
from backend.services.outputs.base import BaseOutputHandler, OutputResult

class MyOutputHandler(BaseOutputHandler):
    def __init__(self, **kwargs):
        self._config = kwargs
    
    async def handle(self, *, ctx, source_doc, processor_result, notify_user):
        # å¤„ç†è¾“å‡ºé€»è¾‘
        # ä¾‹å¦‚ï¼šå‘é€é‚®ä»¶ã€å†™å…¥æ•°æ®åº“ã€æ¨é€é€šçŸ¥ç­‰
        
        return OutputResult(
            child_doc_token=None,
            child_doc_url=None,
            metadata={"output": "my_output"}
        )

# 2. æ³¨å†Œï¼šbackend/services/outputs/registry.py
from backend.services.outputs.my_output import MyOutputHandler

def _make_my_output(feishu: FeishuClient, llm: LLMClient):
    return MyOutputHandler(config_key="value")

OUTPUT_REGISTRY = {
    # ... existing outputs ...
    "my_output": _make_my_output,
}

# 3. é…ç½®ï¼šworkflow_config.yml
workflows:
  my_mode:
    processor: "idea_expander"
    chain: "idea_expand"
    output: "my_output"
    notify_user: false
```

### è°ƒç”¨é£ä¹¦ API

```python
from backend.services.feishu import FeishuClient

feishu = FeishuClient()

# è·å–æ–‡æ¡£å†…å®¹
doc_content = await feishu.get_doc_content("doccnXXXXXXXXXX")

# åˆ›å»ºå­æ–‡æ¡£
child_doc_token = await feishu.create_child_doc(
    folder_token="fldcnXXXXXXXXXX",
    title="AI ç”Ÿæˆçš„æ ‡é¢˜"
)

# å†™å…¥å†…å®¹
await feishu.write_doc_content(
    child_doc_token,
    "# æ ‡é¢˜\n\nå†…å®¹..."
)

# å‘é€å¡ç‰‡
await feishu.send_card(
    user_id="ou_xxxxxxxxxxxxx",
    card_content={
        "header": {"title": {"tag": "plain_text", "content": "æ ‡é¢˜"}},
        "elements": [...]
    }
)
```

### è°ƒç”¨ LLM

```python
from backend.core.llm_client import LLMClient

llm = LLMClient()

# ç®€å•è°ƒç”¨
response = await llm.chat_completion(
    chain="idea_expand",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹"},
        {"role": "user", "content": "è¯·å¸®æˆ‘..."}
    ]
)

# å¸¦å‚æ•°è°ƒç”¨
response = await llm.chat_completion(
    chain="title_generation",
    messages=[...],
    temperature=0.7,
    max_tokens=100
)
```

---

## æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

#### 1. æœåŠ¡å¯åŠ¨å¤±è´¥

**ç—‡çŠ¶**ï¼š
```
ModuleNotFoundError: No module named 'xxx'
```

**è§£å†³**ï¼š
```bash
# å®‰è£…ä¾èµ–
pip install -r backend/requirements.txt
```

---

#### 2. é£ä¹¦ API è°ƒç”¨å¤±è´¥

**ç—‡çŠ¶**ï¼š
```
FeishuAPIError: Failed to get tenant_access_token
```

**æ£€æŸ¥**ï¼š
```bash
# 1. æ£€æŸ¥ .env é…ç½®
cat .env | grep FEISHU

# 2. éªŒè¯ App ID å’Œ Secret
# åœ¨é£ä¹¦å¼€å‘è€…åå°ç¡®è®¤å‡­è¯
```

---

#### 3. LLM è°ƒç”¨è¶…æ—¶

**ç—‡çŠ¶**ï¼š
```
FallbackExhaustedError: All providers failed for chain=xxx
```

**æ£€æŸ¥**ï¼š
```bash
# 1. æ£€æŸ¥ç½‘ç»œè¿æ¥
curl -I https://zjuapi.com

# 2. æ£€æŸ¥ API Key
cat .env | grep API_KEY

# 3. æŸ¥çœ‹æ—¥å¿—
tail -f logs/app.log | grep "LLM"
```

---

#### 4. æ ‡é¢˜ç”Ÿæˆå¤±è´¥

**ç—‡çŠ¶**ï¼š
æ—¥å¿—æ˜¾ç¤º "æ ‡é¢˜ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ ‡é¢˜"

**åŸå› **ï¼š
- LLM API è¶…æ—¶
- Chain é…ç½®é”™è¯¯
- API Key æ— æ•ˆ

**è§£å†³**ï¼š
```yaml
# æ£€æŸ¥ llm_config.yml
chains:
  title_generation:
    - provider: "no_thinking_gemini"
      timeout_s: 15  # ç¡®ä¿è¶…æ—¶è¶³å¤Ÿ
```

---

#### 5. å­æ–‡æ¡£åˆ›å»ºå¤±è´¥

**ç—‡çŠ¶**ï¼š
```
FeishuAPIError: Unable to parse child document token
```

**åŸå› **ï¼š
- æƒé™ä¸è¶³ï¼ˆéœ€è¦ `docx:write` æƒé™ï¼‰
- folder_token æ— æ•ˆ
- Wiki èŠ‚ç‚¹ä¸å­˜åœ¨

**æ£€æŸ¥æƒé™**ï¼š
1. é£ä¹¦å¼€å‘è€…åå° â†’ åº”ç”¨è¯¦æƒ… â†’ æƒé™ç®¡ç†
2. ç¡®è®¤å·²å¼€é€šï¼š
   - `docx:read`
   - `docx:write`
   - `wiki:node:read`ï¼ˆå¦‚æœä½¿ç”¨ Wikiï¼‰
   - `wiki:node:write`ï¼ˆå¦‚æœä½¿ç”¨ Wikiï¼‰

---

#### 6. ä»»åŠ¡çŠ¶æ€æŸ¥è¯¢ 500 é”™è¯¯

**ç—‡çŠ¶**ï¼š
```
PydanticSerializationError: Unable to serialize unknown type: <class 'function'>
```

**åŸå› **ï¼šmetadata ä¸­åŒ…å«ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡ï¼ˆå¦‚å‡½æ•°ï¼‰

**å·²ä¿®å¤**ï¼šv2025-12-31 ç‰ˆæœ¬å·²ä¿®å¤æ­¤é—®é¢˜

---

### æ—¥å¿—æŸ¥çœ‹

#### æŸ¥çœ‹å®æ—¶æ—¥å¿—

```bash
# å¦‚æœä½¿ç”¨ systemd
journalctl -u ai-idea-gen -f

# å¦‚æœä½¿ç”¨ screen/tmux
tail -f logs/app.log
```

#### æŸ¥çœ‹ç‰¹å®šæ¨¡å—æ—¥å¿—

```bash
# LLM è°ƒç”¨æ—¥å¿—
grep "LLM" logs/app.log

# é£ä¹¦ API æ—¥å¿—
grep "Feishu" logs/app.log

# é”™è¯¯æ—¥å¿—
grep "ERROR" logs/app.log
```

---

### è°ƒè¯•æŠ€å·§

#### 1. å¯ç”¨è¯¦ç»†æ—¥å¿—

```python
# backend/main.py
import logging

logging.basicConfig(
    level=logging.DEBUG,  # æ”¹ä¸º DEBUG
    format="%(asctime)s %(name)s %(levelname)s %(message)s"
)
```

#### 2. å•æ­¥æµ‹è¯•

```python
# æµ‹è¯•é£ä¹¦è¿æ¥
from backend.services.feishu import FeishuClient
feishu = FeishuClient()
token = await feishu.get_tenant_access_token()
print(f"Token: {token[:10]}...")

# æµ‹è¯• LLM è°ƒç”¨
from backend.core.llm_client import LLMClient
llm = LLMClient()
result = await llm.chat_completion(
    chain="title_generation",
    messages=[{"role": "user", "content": "Hello"}]
)
print(result)
```

#### 3. ä½¿ç”¨ Python REPL

```bash
# å¯åŠ¨ Python REPL
python3

# å¯¼å…¥æ¨¡å—æµ‹è¯•
from backend.services.feishu import FeishuClient
import asyncio

async def test():
    feishu = FeishuClient()
    content = await feishu.get_doc_content("doccnXXXXXXXXXX")
    print(content[:100])

asyncio.run(test())
```

---

## æ€§èƒ½ç›‘æ§

### å…³é”®æŒ‡æ ‡

- **API å“åº”æ—¶é—´**ï¼š< 200msï¼ˆè§¦å‘æ¥å£ï¼‰
- **ä»»åŠ¡å¤„ç†æ—¶é—´**ï¼š
  - æ€è·¯æ‰©å±•ï¼š30-60 ç§’
  - æ·±åº¦è°ƒç ”ï¼š5-40 åˆ†é’Ÿ
- **Token ç¼“å­˜å‘½ä¸­ç‡**ï¼š> 95%
- **LLM Fallback è§¦å‘ç‡**ï¼š< 5%

### ç›‘æ§å‘½ä»¤

```bash
# æŸ¥çœ‹è¿›ç¨‹çŠ¶æ€
ps aux | grep uvicorn

# æŸ¥çœ‹å†…å­˜ä½¿ç”¨
free -h

# æŸ¥çœ‹ç½‘ç»œè¿æ¥
netstat -antp | grep 8001

# æŸ¥çœ‹ CPU ä½¿ç”¨
top -p $(pgrep -f uvicorn)
```

---

## å¿«é€Ÿå‚è€ƒå¡ç‰‡

### ç›®å½•ç»“æ„é€ŸæŸ¥

```
backend/
â”œâ”€â”€ api/routes.py          # API è·¯ç”±
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ llm_client.py      # LLM å®¢æˆ·ç«¯
â”‚   â”œâ”€â”€ manager.py         # æµç¨‹ç¼–æ’
â”‚   â””â”€â”€ task_store.py      # ä»»åŠ¡å­˜å‚¨
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ processors/        # å¤„ç†å™¨
â”‚   â”œâ”€â”€ outputs/           # è¾“å‡ºå™¨
â”‚   â”œâ”€â”€ utils/             # å·¥å…·å±‚
â”‚   â””â”€â”€ feishu.py          # é£ä¹¦ API
â”œâ”€â”€ config.py              # é…ç½®
â””â”€â”€ main.py                # å…¥å£
```

### é…ç½®æ–‡ä»¶é€ŸæŸ¥

```
.env                       # ç¯å¢ƒå˜é‡
llm_config.yml            # LLM é…ç½®
workflow_config.yml       # å·¥ä½œæµé…ç½®
```

### å¸¸ç”¨ç«¯å£

```
8001                      # FastAPI æœåŠ¡
```

### å…³é”®æ¦‚å¿µ

- **Mode**: å¤„ç†æ¨¡å¼ï¼ˆidea_expand, researchï¼‰
- **Chain**: LLM Provider é“¾ï¼ˆFallbackï¼‰
- **Processor**: æ–‡æ¡£å¤„ç†å™¨ï¼ˆç­–ç•¥æ¨¡å¼ï¼‰
- **OutputHandler**: è¾“å‡ºå¤„ç†å™¨ï¼ˆç­–ç•¥æ¨¡å¼ï¼‰
- **Task**: å¼‚æ­¥ä»»åŠ¡ï¼ˆtask_idï¼‰

---

> ğŸ“ **ç»´æŠ¤è¯´æ˜**ï¼šæœ¬æ–‡æ¡£éšé¡¹ç›®æ›´æ–°ï¼Œè¯·ä¿æŒåŒæ­¥æ›´æ–°
